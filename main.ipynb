{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from utils import *\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 8)\n",
      "(100, 1)\n",
      "body surface are feature added modified\n",
      "body surface are feature added modified\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"TrainningInput_Dataset_myseg.csv\",index_col = 0)\n",
    "print(X.shape)\n",
    "\n",
    "y = pd.read_csv(\"TrainningOutput_Dataset_myseg.csv\",index_col=0)\n",
    "print(y.shape)\n",
    "\n",
    "X_test = pd.read_csv(\"TestingInput_Dataset_myseg.csv\",index_col=0)\n",
    "\n",
    "# Shuffle the data\n",
    "X = X.sample(n=X.shape[0])\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# Here create other feature from the original ones. Maybe compute body surface, first i did imc but what I really wanted was like the average organ size of a patient so I took instead a measure of body area.. Then remove the feature height and weight.\n",
    "add_body_surface_area_feature(X)\n",
    "X.drop(columns=[\"Height\", \"Weight\"],axis=1,inplace=True)\n",
    "add_ratio_features(X)\n",
    "\n",
    "add_body_surface_area_feature(X_test)\n",
    "X_test.drop(columns=[\"Height\",\"Weight\"],axis =1,inplace = True)\n",
    "add_ratio_features(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'transform_input', 'verbose', 'dataAugment', 'normaliser', 'classifier', 'dataAugment__noise_factor', 'dataAugment__random_state', 'normaliser__clip', 'normaliser__copy', 'normaliser__feature_range', 'classifier__bootstrap', 'classifier__ccp_alpha', 'classifier__class_weight', 'classifier__criterion', 'classifier__max_depth', 'classifier__max_features', 'classifier__max_leaf_nodes', 'classifier__max_samples', 'classifier__min_impurity_decrease', 'classifier__min_samples_leaf', 'classifier__min_samples_split', 'classifier__min_weight_fraction_leaf', 'classifier__monotonic_cst', 'classifier__n_estimators', 'classifier__n_jobs', 'classifier__oob_score', 'classifier__random_state', 'classifier__verbose', 'classifier__warm_start'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = ImbPipeline([\n",
    "    (\"dataAugment\",GaussianNoiseInjector()),\n",
    "    (\"normaliser\" , MinMaxScaler()),\n",
    "    (\"classifier\", RandomForestClassifier()),\n",
    "])\n",
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'dataAugment__noise_factor' : [0.05,0.0],\n",
    "    'classifier__n_estimators': [100,300,500,1000],\n",
    "    'classifier__max_features': ['sqrt',0.1,0.3],\n",
    "    'classifier__max_depth': [5,15],\n",
    "    'classifier__min_samples_split': [2],\n",
    "    'classifier__min_samples_leaf': [2],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.850) total time=   0.4s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   0.6s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.5s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   0.4s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.850) total time=   0.4s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.4s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.950) total time=   0.3s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.750) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.850) total time=   1.0s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   1.3s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   2.2s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.950) total time=   1.5s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   1.3s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.850) total time=   1.0s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.0s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.9s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.9s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.750) total time=   1.4s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.850) total time=   1.6s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   1.6s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   1.2s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   1.6s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   1.9s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.850) total time=   2.3s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.2s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.8s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   1.7s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.800) total time=   1.7s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.850) total time=   3.1s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.950) total time=   3.2s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   2.3s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   2.3s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.750) total time=   2.3s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.850) total time=   2.3s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.3s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   3.3s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.5s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.800) total time=   2.4s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.850) total time=   0.2s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   0.2s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.850) total time=   0.2s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.2s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.2s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.2s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   0.5s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.850) total time=   0.7s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   0.8s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.7s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.7s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   0.7s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.850) total time=   0.7s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.7s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.7s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.7s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.800) total time=   0.7s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.850) total time=   1.1s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   1.3s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   1.1s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   1.1s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.900) total time=   1.2s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.850) total time=   1.2s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   1.1s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.2s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.0s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.800) total time=   1.7s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.850) total time=   2.9s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   3.3s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=0.1, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   4.0s\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline,param_grid=param_grid,cv=5,verbose=3,return_train_score=True)\n",
    "grid_search.fit(X,y[\"Category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fitted and now we want to properly evaluate the results. \n",
    "We select only the result from the best_params founded by the search. (Reminder that the best params are the one that provided the best mean validation score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100, 'dataAugment__noise_factor': 0.0}\n",
      "score validation set : (np.float64(0.8099999999999999), np.float64(0.07348469228349536))\n",
      "score trainning set : (np.float64(1.0), np.float64(0.0))\n"
     ]
    }
   ],
   "source": [
    "# set of parameters that gave the best cv result\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"best parameters : {best_params} \")\n",
    "\n",
    "# Detailed result of the cross validation for each set of parameters\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# CV result for the best paramaters.\n",
    "\n",
    "# GOAL : \n",
    "# The closest to 1 the mean score is on the val set the better\n",
    "# The smallest the std on the val set the better.\n",
    "best_idx = results['params'].index(best_params)\n",
    "mean_train_score = results['mean_train_score'][best_idx]\n",
    "mean_valid_score = results['mean_test_score'][best_idx]\n",
    "std_train_score = results['std_train_score'][best_idx]\n",
    "std_valid_score = results['std_test_score'][best_idx]\n",
    "print(f\"score validation set : {mean_valid_score,std_valid_score}\")\n",
    "print(f\"score trainning set : {mean_train_score,std_train_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('normaliser', StandardScaler()), ('classifier', RandomForestClassifier(max_depth=15, min_samples_leaf=2))], 'transform_input': None, 'verbose': False, 'normaliser': StandardScaler(), 'classifier': RandomForestClassifier(max_depth=15, min_samples_leaf=2), 'normaliser__copy': True, 'normaliser__with_mean': True, 'normaliser__with_std': True, 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 15, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__monotonic_cst': None, 'classifier__n_estimators': 100, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# CONSTRUCTION DE LA PIPELINE D\"INFERENCE\n",
    "\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "inference_steps = [\n",
    "    (name, step)\n",
    "    for name, step in best_pipeline.steps\n",
    "    if name != \"dataAugment\" # on ne bruite plus les donnÃ©es \n",
    "]\n",
    "inf_pipeline = Pipeline(inference_steps)\n",
    "print(inf_pipeline.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_importance = grid_search.best_estimator_.named_steps[\"classifier\"].feature_importances_\n",
    "f_name = grid_search.best_estimator_.named_steps[\"normaliser\"].get_feature_names_out()\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": f_name,\n",
    "    \"importance\": f_importance\n",
    "})\n",
    "feature_importance.sort_values(\"importance\", ascending=False, inplace=True)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "[[20  0  0  0  0]\n",
      " [ 0 19  1  0  0]\n",
      " [ 0  0 20  0  0]\n",
      " [ 0  0  0 20  0]\n",
      " [ 0  0  0  0 20]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "# Get global trainning score on the whole trainning data \n",
    "y_pred = inf_pipeline.predict(X)\n",
    "cm = confusion_matrix(y,y_pred)\n",
    "acc = accuracy_score(y,y_pred)\n",
    "print(acc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0325     0.0075     0.015      0.         0.945     ]\n",
      " [0.29983333 0.09083333 0.04583333 0.0425     0.521     ]\n",
      " [0.01       0.07866667 0.039      0.         0.87233333]\n",
      " [0.         0.         0.         1.         0.        ]\n",
      " [0.11416667 0.01       0.         0.8675     0.00833333]\n",
      " [0.03333333 0.         0.         0.96166667 0.005     ]\n",
      " [0.74       0.005      0.         0.21833333 0.03666667]\n",
      " [0.99666667 0.         0.         0.         0.00333333]\n",
      " [0.00333333 0.18       0.78666667 0.005      0.025     ]\n",
      " [0.00333333 0.002      0.         0.016      0.97866667]\n",
      " [0.         0.02833333 0.01666667 0.955      0.        ]\n",
      " [0.18583333 0.005      0.         0.78166667 0.0275    ]\n",
      " [0.         0.14833333 0.83416667 0.0075     0.01      ]\n",
      " [0.98       0.         0.         0.         0.02      ]\n",
      " [0.965      0.         0.         0.00333333 0.03166667]\n",
      " [0.         0.         0.005      0.         0.995     ]\n",
      " [0.         0.17583333 0.82416667 0.         0.        ]\n",
      " [0.         0.9575     0.02416667 0.01833333 0.        ]\n",
      " [0.09333333 0.         0.         0.         0.90666667]\n",
      " [0.03833333 0.         0.         0.95166667 0.01      ]\n",
      " [0.         0.         0.01       0.         0.99      ]\n",
      " [0.754      0.08833333 0.01666667 0.015      0.126     ]\n",
      " [0.         0.86333333 0.13666667 0.         0.        ]\n",
      " [0.         0.03       0.95       0.005      0.015     ]\n",
      " [0.         0.02666667 0.97333333 0.         0.        ]\n",
      " [0.01083333 0.0025     0.         0.         0.98666667]\n",
      " [0.         0.04083333 0.95666667 0.0025     0.        ]\n",
      " [0.92916667 0.005      0.         0.005      0.06083333]\n",
      " [0.0475     0.92333333 0.00666667 0.01       0.0125    ]\n",
      " [0.         0.005      0.         0.995      0.        ]\n",
      " [0.01       0.         0.         0.         0.99      ]\n",
      " [0.         0.         0.         0.         1.        ]\n",
      " [0.         0.89583333 0.10416667 0.         0.        ]\n",
      " [0.         0.93416667 0.06583333 0.         0.        ]\n",
      " [0.055      0.89166667 0.         0.05       0.00333333]\n",
      " [0.         0.03466667 0.96333333 0.         0.002     ]\n",
      " [0.0375     0.01       0.         0.9525     0.        ]\n",
      " [0.94833333 0.         0.         0.00333333 0.04833333]\n",
      " [0.12166667 0.09683333 0.12       0.60116667 0.06033333]\n",
      " [0.11666667 0.7265     0.024      0.11833333 0.0145    ]\n",
      " [0.775      0.05       0.         0.175      0.        ]\n",
      " [0.         0.055      0.9375     0.0075     0.        ]\n",
      " [0.         0.         0.         1.         0.        ]\n",
      " [0.         0.005      0.         0.995      0.        ]\n",
      " [0.         0.33366667 0.65633333 0.01       0.        ]\n",
      " [0.01       0.         0.         0.99       0.        ]\n",
      " [0.         0.12       0.005      0.875      0.        ]\n",
      " [0.03333333 0.         0.         0.         0.96666667]\n",
      " [0.0325     0.40766667 0.3785     0.12783333 0.0535    ]\n",
      " [0.         0.42166667 0.57833333 0.         0.        ]\n",
      " [0.04583333 0.015      0.         0.9225     0.01666667]\n",
      " [0.005      0.         0.         0.         0.995     ]\n",
      " [0.         0.05416667 0.93083333 0.01       0.005     ]\n",
      " [0.         0.19983333 0.79016667 0.01       0.        ]\n",
      " [0.93166667 0.         0.         0.065      0.00333333]\n",
      " [1.         0.         0.         0.         0.        ]\n",
      " [0.         0.92583333 0.07416667 0.         0.        ]\n",
      " [0.25833333 0.         0.005      0.         0.73666667]\n",
      " [0.         0.91416667 0.08583333 0.         0.        ]\n",
      " [0.02416667 0.2845     0.59966667 0.00833333 0.08333333]\n",
      " [0.06083333 0.015      0.0225     0.         0.90166667]\n",
      " [0.01333333 0.0525     0.87083333 0.005      0.05833333]\n",
      " [0.         0.95416667 0.04583333 0.         0.        ]\n",
      " [0.         0.78916667 0.21083333 0.         0.        ]\n",
      " [0.         0.02       0.         0.98       0.        ]\n",
      " [0.         0.985      0.01       0.005      0.        ]\n",
      " [0.0275     0.0075     0.         0.         0.965     ]\n",
      " [0.03916667 0.92366667 0.02216667 0.01       0.005     ]\n",
      " [0.         0.045      0.955      0.         0.        ]\n",
      " [0.005      0.005      0.         0.9775     0.0125    ]\n",
      " [0.         0.05833333 0.87166667 0.         0.07      ]\n",
      " [0.         0.1975     0.8025     0.         0.        ]\n",
      " [0.985      0.         0.         0.         0.015     ]\n",
      " [0.04333333 0.92333333 0.02833333 0.         0.005     ]\n",
      " [0.915      0.025      0.         0.01666667 0.04333333]\n",
      " [0.87083333 0.005      0.         0.11416667 0.01      ]\n",
      " [0.         0.04416667 0.94083333 0.01       0.005     ]\n",
      " [0.01833333 0.002      0.         0.08016667 0.8995    ]\n",
      " [0.02       0.007      0.         0.041      0.932     ]\n",
      " [0.         0.         0.         1.         0.        ]\n",
      " [0.09416667 0.86833333 0.         0.0325     0.005     ]\n",
      " [0.779      0.0975     0.00333333 0.0175     0.10266667]\n",
      " [0.97833333 0.015      0.         0.         0.00666667]\n",
      " [1.         0.         0.         0.         0.        ]\n",
      " [0.01333333 0.79783333 0.169      0.0025     0.01733333]\n",
      " [0.3625     0.03       0.00833333 0.59083333 0.00833333]\n",
      " [0.         0.         0.         1.         0.        ]\n",
      " [0.99333333 0.         0.         0.         0.00666667]\n",
      " [0.         0.145      0.845      0.01       0.        ]\n",
      " [0.80833333 0.01       0.         0.17666667 0.005     ]\n",
      " [0.01       0.         0.005      0.         0.985     ]\n",
      " [0.         0.13483333 0.86516667 0.         0.        ]\n",
      " [0.04583333 0.81716667 0.09916667 0.01583333 0.022     ]\n",
      " [0.         0.1285     0.8645     0.005      0.002     ]\n",
      " [0.02333333 0.         0.         0.97666667 0.        ]\n",
      " [0.95333333 0.02       0.01       0.01666667 0.        ]\n",
      " [0.00833333 0.         0.         0.         0.99166667]\n",
      " [0.96166667 0.         0.         0.00333333 0.035     ]\n",
      " [0.01166667 0.96333333 0.02       0.         0.005     ]\n",
      " [0.0275     0.0075     0.015      0.         0.95      ]]\n",
      "[0, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Get the probabilities to get better insight on the model performance\n",
    "proba = inf_pipeline.predict_proba(X)\n",
    "\n",
    "\n",
    "# Below \n",
    "treshold = 0.4\n",
    "L = [0,0,0,0,0]\n",
    "for x in proba : \n",
    "    a= 0\n",
    "    index = []\n",
    "    for ind,i in enumerate(x) :\n",
    "        if i > treshold : \n",
    "            a+=1\n",
    "            index.append(ind)\n",
    "    if a >=2 : \n",
    "        for j in index : \n",
    "            L[j] +=1  \n",
    "print(proba)  \n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "\n",
    "submission_name = \"submission_10.csv\"\n",
    "submission_dataframe = pd.DataFrame(columns=[\"Id\",\"Category\"])\n",
    "submission_dataframe[\"Id\"] = X_test.index + 101\n",
    "\n",
    "y_test_pred = inf_pipeline.predict(X_test)\n",
    "submission_dataframe[\"Category\"] = y_test_pred\n",
    "submission_dataframe.to_csv(os.path.join(os.getcwd(),submission_name),index=False)\n",
    "\n",
    "print(\"File saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below two cells to save the results : the model and the description of the method used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"Data augmentation cleaned + MinmaxScaler + Randomforest with Gridsearch.\" \n",
    "other_params = \"The features are just the volume of each segmentation + body surface + all the possible ratios.\"\n",
    "name_folder = \"RF_data_aug_noise_pipeline\"\n",
    "feature_used = f_name\n",
    "informationDict = {\n",
    "    \"description\": description,\n",
    "    \"model parameters\" : best_params,\n",
    "    \"features used\" : feature_used,\n",
    "    \"mean test accuracy with best params\" : mean_valid_score ,\n",
    "    \"std  test with best params\" : std_valid_score,\n",
    "    \"mean train accuracy with best params\" : mean_train_score,\n",
    "    \"std train best params\" : std_train_score,\n",
    "    \"other parms\" : other_params,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: /Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/output/RF_data_aug_noise_pipeline/pipeline_01-08-15.pkl\n",
      "Information about the model saved to: /Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/output/RF_data_aug_noise_pipeline/params.txt\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create a timestamp\n",
    "currentDateTime = datetime.now()\n",
    "\n",
    "# Get the base directory (current directory)\n",
    "BASE_DIR = os.getcwd()\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "RESULT_DIR = os.path.join(BASE_DIR,\"output\")\n",
    "if not os.path.exists(RESULT_DIR):\n",
    "    os.mkdir(RESULT_DIR)\n",
    "    print(f\"Directory created: {RESULT_DIR}\")\n",
    "\n",
    "# Create a folder named 'pipeline_<timestamp>' in the current directory\n",
    "dir_name = name_folder\n",
    "dir_path = os.path.join(RESULT_DIR, dir_name)\n",
    "\n",
    "# If the directory doesn't exist, create it\n",
    "if not os.path.exists(dir_path):\n",
    "    os.mkdir(dir_path)\n",
    "    print(f\"Directory created: {dir_path}\")\n",
    "\n",
    "# Save the model inside this new folder\n",
    "model_filename = 'pipeline_' + currentDateTime.strftime(\"%H-%M-%S\") + '.pkl'\n",
    "model_path = os.path.join(dir_path, model_filename)\n",
    "\n",
    "# This is where you'd have your model defined\n",
    "joblib.dump(grid_search, model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "\n",
    "# Saving feature importance : \n",
    "feature_importance_filename ='pipeline_' + currentDateTime.strftime(\"%H-%M-%S\") + '.csv' \n",
    "feature_importance_dir = os.path.join(dir_path,feature_importance_filename)\n",
    "feature_importance.to_csv(feature_importance_dir)\n",
    "\n",
    "\n",
    "# SAVING Description \n",
    "dict_filename = 'params.txt'\n",
    "dict_path = os.path.join(dir_path, dict_filename)\n",
    "\n",
    "with open(dict_path, 'w') as f:\n",
    "    for key, val in informationDict.items():\n",
    "        f.write(f\"{key} : {val}\\n\")\n",
    "print(f\"Information about the model saved to: {dict_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
