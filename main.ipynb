{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from utils import *\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 14)\n",
      "(100, 1)\n",
      "body surface are feature added modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body surface are feature added modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n",
      "/Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/utils.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_col_name] = df[col1] / df[col2].replace(0, float('nan'))\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"TrainningInput_Dataset_myseg_area.csv\",index_col = 0)\n",
    "print(X.shape)\n",
    "\n",
    "y = pd.read_csv(\"TrainningOutput_Dataset_myseg_area.csv\",index_col=0)\n",
    "print(y.shape)\n",
    "\n",
    "X_test = pd.read_csv(\"TestingInput_Dataset_myseg_area.csv\",index_col=0)\n",
    "\n",
    "# Shuffle the data\n",
    "X = X.sample(n=X.shape[0])\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# Here create other feature from the original ones. Maybe compute body surface, first i did imc but what I really wanted was like the average organ size of a patient so I took instead a measure of body area.. Then remove the feature height and weight.\n",
    "add_body_surface_area_feature(X)\n",
    "X.drop(columns=[\"Height\", \"Weight\"],axis=1,inplace=True)\n",
    "add_ratio_features(X)\n",
    "\n",
    "add_body_surface_area_feature(X_test)\n",
    "X_test.drop(columns=[\"Height\",\"Weight\"],axis =1,inplace = True)\n",
    "add_ratio_features(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'transform_input', 'verbose', 'normaliser', 'dataAugment', 'classifier', 'normaliser__clip', 'normaliser__copy', 'normaliser__feature_range', 'dataAugment__noise_factor', 'dataAugment__random_state', 'classifier__bootstrap', 'classifier__ccp_alpha', 'classifier__class_weight', 'classifier__criterion', 'classifier__max_depth', 'classifier__max_features', 'classifier__max_leaf_nodes', 'classifier__max_samples', 'classifier__min_impurity_decrease', 'classifier__min_samples_leaf', 'classifier__min_samples_split', 'classifier__min_weight_fraction_leaf', 'classifier__monotonic_cst', 'classifier__n_estimators', 'classifier__n_jobs', 'classifier__oob_score', 'classifier__random_state', 'classifier__verbose', 'classifier__warm_start'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = ImbPipeline([\n",
    "    (\"normaliser\" , MinMaxScaler()),\n",
    "    (\"dataAugment\",GaussianNoiseInjector()),\n",
    "    (\"classifier\", RandomForestClassifier()),\n",
    "])\n",
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'dataAugment__noise_factor' : [0.05,0.0],\n",
    "    'classifier__n_estimators': [100,300,500,1000],\n",
    "    'classifier__max_features': ['sqrt'],\n",
    "    'classifier__max_depth': [5,15],\n",
    "    'classifier__min_samples_split': [2,5],\n",
    "    'classifier__min_samples_leaf': [2,5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.950) total time=   0.9s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   0.3s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.6s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   0.4s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.800) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.5s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=1.000) total time=   0.5s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   0.5s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.750) total time=   0.5s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   1.3s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   0.8s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.8s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.850) total time=   1.0s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.750) total time=   0.9s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   1.0s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.9s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.8s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.800) total time=   0.8s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   0.9s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   1.4s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   1.4s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   1.2s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.900) total time=   1.4s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.800) total time=   1.3s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   1.4s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   1.5s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.3s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   1.3s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.750) total time=   1.4s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   2.6s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   2.5s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   2.4s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   2.4s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.800) total time=   2.7s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.7s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.8s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   3.6s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   3.1s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   2.9s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.900) total time=   0.3s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.4s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   0.4s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.800) total time=   0.6s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.3s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.5s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.6s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   0.3s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   0.5s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   1.6s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   1.1s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.9s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   1.1s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.750) total time=   1.5s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   1.2s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.3s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.1s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   1.1s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.750) total time=   1.0s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.950) total time=   2.1s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   3.1s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   5.1s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   2.6s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.850) total time=   1.8s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   1.5s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   1.7s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.4s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   1.4s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   1.4s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   2.9s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   2.7s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   2.8s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   2.7s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.800) total time=   2.8s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.7s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.7s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   2.7s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.800) total time=   2.7s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   2.9s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.950) total time=   0.3s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.950) total time=   0.3s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   0.3s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.750) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.3s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.3s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.975, test=1.000) total time=   0.3s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   0.3s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   0.8s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   0.9s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.8s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   0.8s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.750) total time=   0.8s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.8s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.8s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.8s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.800) total time=   0.8s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.750) total time=   0.8s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.950) total time=   1.3s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   1.4s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   1.4s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   1.4s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.750) total time=   1.4s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   1.3s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.3s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.4s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.800) total time=   1.6s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   1.3s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   2.7s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   2.6s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   2.6s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   2.7s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.800) total time=   2.6s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   3.2s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   3.6s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   3.2s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.800) total time=   3.6s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.750) total time=   3.2s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   0.3s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   0.3s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   0.3s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.3s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   0.3s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.750) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.950) total time=   0.8s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   0.9s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   1.0s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.800) total time=   1.0s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   0.9s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   1.0s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.8s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.8s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.800) total time=   0.8s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   0.8s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   1.5s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.950) total time=   2.9s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   1.9s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   2.1s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.850) total time=   2.1s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   3.4s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.8s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.4s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   1.4s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.750) total time=   1.4s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   2.9s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   2.7s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   2.7s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   2.7s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.800) total time=   2.6s\n",
      "[CV 1/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.6s\n",
      "[CV 2/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.7s\n",
      "[CV 3/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   2.7s\n",
      "[CV 4/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   2.9s\n",
      "[CV 5/5] END classifier__max_depth=5, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.750) total time=   2.7s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   0.3s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.900) total time=   0.3s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.800) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=1.000) total time=   0.3s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.3s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=1.000) total time=   0.3s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   0.3s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.750) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   0.8s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.9s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.8s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   0.8s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.850) total time=   0.9s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.9s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.9s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=1.000, test=1.000) total time=   0.9s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.850) total time=   0.9s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   0.8s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   1.4s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   1.4s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   1.4s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   1.4s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.800) total time=   1.4s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.1s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=1.000, test=1.000) total time=   1.8s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=1.000, test=1.000) total time=   1.4s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   1.4s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   1.5s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   3.8s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   2.8s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   2.8s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   2.8s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.850) total time=   2.7s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.7s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.950) total time=   2.7s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=1.000, test=1.000) total time=   4.0s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   2.8s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   2.8s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   0.3s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   0.3s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.800) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=1.000) total time=   0.3s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.800) total time=   0.3s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.750) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   0.9s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.8s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.8s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   0.8s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.800) total time=   0.8s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.8s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.9s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.9s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   0.8s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   0.8s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   1.4s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   1.4s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   1.4s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   1.5s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.800) total time=   1.4s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   1.4s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   1.4s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.4s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   1.4s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   1.4s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.950) total time=   2.7s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   2.8s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   2.7s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   3.1s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.800) total time=   2.8s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.7s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.7s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   2.9s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   3.5s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   3.7s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.900) total time=   1.2s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.950) total time=   0.5s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.4s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   0.5s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.750) total time=   0.5s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.8s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.3s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.4s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   0.4s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.750) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.950) total time=   1.1s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   1.4s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   1.1s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   1.9s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.800) total time=   1.1s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.8s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.9s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.0s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.800) total time=   1.5s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.750) total time=   1.4s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.963, test=0.950) total time=   3.2s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   1.7s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   1.8s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   1.6s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.750) total time=   1.4s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   1.6s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.4s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.4s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   1.4s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   1.4s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.950) total time=   2.7s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   2.8s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   4.2s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   4.3s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.800) total time=   5.0s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   3.6s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   4.0s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   3.2s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   2.7s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=2, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   2.7s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.963, test=0.950) total time=   0.3s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.950) total time=   0.3s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   0.3s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.3s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.975, test=1.000) total time=   0.4s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.800) total time=   0.3s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=100, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.750) total time=   0.3s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.963, test=0.950) total time=   0.8s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   0.9s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   1.1s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.850) total time=   1.0s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   0.8s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.9s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   0.8s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   0.8s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   0.8s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=300, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   0.8s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.950) total time=   1.4s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   1.4s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   1.4s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   1.6s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.750) total time=   1.4s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.950) total time=   2.3s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.8s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   1.4s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   1.4s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=500, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.800) total time=   1.4s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.975, test=0.950) total time=   2.7s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=1.000) total time=   2.9s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.975, test=1.000) total time=   2.9s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=0.988, test=0.800) total time=   2.7s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.05;, score=(train=1.000, test=0.750) total time=   2.7s\n",
      "[CV 1/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   2.6s\n",
      "[CV 2/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   2.9s\n",
      "[CV 3/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=1.000) total time=   3.2s\n",
      "[CV 4/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=0.988, test=0.850) total time=   3.1s\n",
      "[CV 5/5] END classifier__max_depth=15, classifier__max_features=sqrt, classifier__min_samples_leaf=5, classifier__min_samples_split=5, classifier__n_estimators=1000, dataAugment__noise_factor=0.0;, score=(train=1.000, test=0.750) total time=   3.6s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;normaliser&#x27;, MinMaxScaler()),\n",
       "                                       (&#x27;dataAugment&#x27;, GaussianNoiseInjector()),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        RandomForestClassifier())]),\n",
       "             param_grid={&#x27;classifier__max_depth&#x27;: [5, 15],\n",
       "                         &#x27;classifier__max_features&#x27;: [&#x27;sqrt&#x27;],\n",
       "                         &#x27;classifier__min_samples_leaf&#x27;: [2, 5],\n",
       "                         &#x27;classifier__min_samples_split&#x27;: [2, 5],\n",
       "                         &#x27;classifier__n_estimators&#x27;: [100, 300, 500, 1000],\n",
       "                         &#x27;dataAugment__noise_factor&#x27;: [0.05, 0.0]},\n",
       "             return_train_score=True, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;normaliser&#x27;, MinMaxScaler()),\n",
       "                                       (&#x27;dataAugment&#x27;, GaussianNoiseInjector()),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        RandomForestClassifier())]),\n",
       "             param_grid={&#x27;classifier__max_depth&#x27;: [5, 15],\n",
       "                         &#x27;classifier__max_features&#x27;: [&#x27;sqrt&#x27;],\n",
       "                         &#x27;classifier__min_samples_leaf&#x27;: [2, 5],\n",
       "                         &#x27;classifier__min_samples_split&#x27;: [2, 5],\n",
       "                         &#x27;classifier__n_estimators&#x27;: [100, 300, 500, 1000],\n",
       "                         &#x27;dataAugment__noise_factor&#x27;: [0.05, 0.0]},\n",
       "             return_train_score=True, verbose=3)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: Pipeline</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;normaliser&#x27;, MinMaxScaler()),\n",
       "                (&#x27;dataAugment&#x27;, GaussianNoiseInjector(noise_factor=0.05)),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 RandomForestClassifier(max_depth=5, min_samples_leaf=2,\n",
       "                                        n_estimators=500))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MinMaxScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.MinMaxScaler.html\">?<span>Documentation for MinMaxScaler</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>MinMaxScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GaussianNoiseInjector</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>GaussianNoiseInjector(noise_factor=0.05)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(max_depth=5, min_samples_leaf=2, n_estimators=500)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('normaliser', MinMaxScaler()),\n",
       "                                       ('dataAugment', GaussianNoiseInjector()),\n",
       "                                       ('classifier',\n",
       "                                        RandomForestClassifier())]),\n",
       "             param_grid={'classifier__max_depth': [5, 15],\n",
       "                         'classifier__max_features': ['sqrt'],\n",
       "                         'classifier__min_samples_leaf': [2, 5],\n",
       "                         'classifier__min_samples_split': [2, 5],\n",
       "                         'classifier__n_estimators': [100, 300, 500, 1000],\n",
       "                         'dataAugment__noise_factor': [0.05, 0.0]},\n",
       "             return_train_score=True, verbose=3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline,param_grid=param_grid,cv=5,verbose=3,return_train_score=True)\n",
    "grid_search.fit(X,y[\"Category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fitted and now we want to properly evaluate the results. \n",
    "We select only the result from the best_params founded by the search. (Reminder that the best params are the one that provided the best mean validation score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 500, 'dataAugment__noise_factor': 0.05}\n",
      "score validation set : (np.float64(0.93), np.float64(0.0748331477354788))\n",
      "score trainning set : (np.float64(0.99), np.float64(0.004999999999999982))\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(best_params)\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# Only keep the cv result for the best paramaters.\n",
    "best_idx = results['params'].index(best_params)\n",
    "\n",
    "mean_train_score = results['mean_train_score'][best_idx]\n",
    "mean_valid_score = results['mean_test_score'][best_idx]\n",
    "std_train_score = results['std_train_score'][best_idx]\n",
    "std_valid_score = results['std_test_score'][best_idx]\n",
    "print(f\"score validation set : {mean_valid_score,std_valid_score}\")\n",
    "print(f\"score trainning set : {mean_train_score,std_train_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('normaliser', MinMaxScaler()), ('classifier', RandomForestClassifier(max_depth=5, min_samples_leaf=2, n_estimators=500))], 'transform_input': None, 'verbose': False, 'normaliser': MinMaxScaler(), 'classifier': RandomForestClassifier(max_depth=5, min_samples_leaf=2, n_estimators=500), 'normaliser__clip': False, 'normaliser__copy': True, 'normaliser__feature_range': (0, 1), 'classifier__bootstrap': True, 'classifier__ccp_alpha': 0.0, 'classifier__class_weight': None, 'classifier__criterion': 'gini', 'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__max_leaf_nodes': None, 'classifier__max_samples': None, 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__min_weight_fraction_leaf': 0.0, 'classifier__monotonic_cst': None, 'classifier__n_estimators': 500, 'classifier__n_jobs': None, 'classifier__oob_score': False, 'classifier__random_state': None, 'classifier__verbose': 0, 'classifier__warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# CONSTRUCTION DE LA PIPELINE D\"INFERENCE QUI NE FAIT PLUS DE DATA AUG\n",
    "\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "inference_steps = [\n",
    "    (name, step)\n",
    "    for name, step in best_pipeline.steps\n",
    "    if name != \"dataAugment\"\n",
    "]\n",
    "inf_pipeline = Pipeline(inference_steps)\n",
    "print(inf_pipeline.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "[[20  0  0  0  0]\n",
      " [ 0 19  1  0  0]\n",
      " [ 0  0 20  0  0]\n",
      " [ 0  0  0 20  0]\n",
      " [ 0  0  0  0 20]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "y_pred = inf_pipeline.predict(X)\n",
    "cm = confusion_matrix(y,y_pred)\n",
    "acc = accuracy_score(y,y_pred)\n",
    "print(acc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.28571429e-03 3.00000000e-03 6.66666667e-04 9.87047619e-01\n",
      "  0.00000000e+00]\n",
      " [8.54329004e-03 8.51892419e-01 7.19035600e-02 5.36514816e-02\n",
      "  1.40092492e-02]\n",
      " [1.66666667e-03 1.39463577e-01 8.43903206e-01 1.57777778e-03\n",
      "  1.33887730e-02]\n",
      " [9.53820202e-01 9.09259259e-03 6.66666667e-04 2.79575758e-02\n",
      "  8.46296296e-03]\n",
      " [1.62797980e-01 5.14814815e-03 3.60000000e-03 7.42424242e-03\n",
      "  8.21029630e-01]\n",
      " [7.15222222e-02 1.00481481e-02 5.00000000e-03 4.76666667e-02\n",
      "  8.65762963e-01]\n",
      " [8.12265512e-03 8.48995721e-01 1.38405020e-01 1.85783085e-03\n",
      "  2.61877303e-03]\n",
      " [9.04235888e-01 2.39657952e-02 2.66666667e-03 4.12020202e-02\n",
      "  2.79296296e-02]\n",
      " [2.72337662e-02 6.66921287e-01 2.89256751e-01 1.33306600e-02\n",
      "  3.25753623e-03]\n",
      " [9.78757576e-02 1.43333333e-02 1.00000000e-03 8.54557576e-01\n",
      "  3.22333333e-02]\n",
      " [8.04559885e-02 8.34663405e-01 4.91706698e-02 2.74578309e-02\n",
      "  8.25210637e-03]\n",
      " [7.81666667e-02 3.28000000e-02 4.33333333e-02 4.46666667e-03\n",
      "  8.41233333e-01]\n",
      " [7.00000000e-03 4.88888889e-03 6.66666667e-04 9.86777778e-01\n",
      "  6.66666667e-04]\n",
      " [2.11966667e-01 2.27333333e-02 1.33333333e-03 7.20933333e-01\n",
      "  4.30333333e-02]\n",
      " [3.59242424e-02 7.24029104e-01 2.11939732e-01 4.98481498e-03\n",
      "  2.31221064e-02]\n",
      " [1.07233333e-01 8.70156863e-02 4.15333333e-02 1.83333333e-03\n",
      "  7.62384314e-01]\n",
      " [1.56709957e-03 1.90275561e-01 8.04575102e-01 2.57777778e-03\n",
      "  1.00445931e-03]\n",
      " [8.52223623e-01 3.22657952e-02 8.50000000e-03 2.56666667e-02\n",
      "  8.13439153e-02]\n",
      " [5.85757576e-02 2.82666667e-02 7.73333333e-03 9.09090909e-05\n",
      "  9.05333333e-01]\n",
      " [8.60000000e-02 8.33333333e-03 2.52000000e-02 1.33333333e-03\n",
      "  8.79133333e-01]\n",
      " [1.53538341e-01 7.32245758e-01 3.64175710e-02 4.59068505e-02\n",
      "  3.18914796e-02]\n",
      " [1.07366667e-01 3.73333333e-03 4.20000000e-03 3.73333333e-03\n",
      "  8.80966667e-01]\n",
      " [4.59666667e-02 3.48000000e-02 4.13333333e-02 2.50000000e-03\n",
      "  8.75400000e-01]\n",
      " [1.00000000e-03 3.83508442e-02 9.56220398e-01 1.64444444e-03\n",
      "  2.78431373e-03]\n",
      " [1.59559885e-02 9.00659610e-01 7.89077973e-02 3.85783085e-03\n",
      "  6.18773034e-04]\n",
      " [1.65000000e-02 5.00492802e-02 8.74750720e-01 2.53333333e-03\n",
      "  5.61666667e-02]\n",
      " [7.57575758e-04 8.34129640e-01 1.60064752e-01 2.42925942e-03\n",
      "  2.61877303e-03]\n",
      " [9.01386869e-01 6.25925926e-03 1.20000000e-03 2.07909091e-02\n",
      "  7.03629630e-02]\n",
      " [7.31523810e-02 2.29000000e-02 6.83333333e-03 8.94014286e-01\n",
      "  3.10000000e-03]\n",
      " [3.33333333e-03 3.33333333e-03 0.00000000e+00 9.93333333e-01\n",
      "  0.00000000e+00]\n",
      " [2.56861472e-02 8.69409057e-01 2.59763121e-02 6.95192344e-02\n",
      "  9.40924922e-03]\n",
      " [1.88033333e-01 3.91148148e-02 1.82000000e-02 1.42666667e-02\n",
      "  7.40385185e-01]\n",
      " [2.66666667e-03 2.73333333e-02 6.83333333e-03 9.62166667e-01\n",
      "  1.00000000e-03]\n",
      " [1.04329004e-03 8.55474403e-01 1.28401576e-01 6.81814831e-03\n",
      "  8.26258256e-03]\n",
      " [4.70194805e-02 8.85532247e-01 5.14340746e-02 7.87161531e-03\n",
      "  8.14258256e-03]\n",
      " [8.19102555e-01 5.54357298e-03 0.00000000e+00 1.61924242e-01\n",
      "  1.34296296e-02]\n",
      " [5.00000000e-03 8.29940464e-02 9.03161509e-01 2.44444444e-04\n",
      "  8.60000000e-03]\n",
      " [9.71153535e-01 3.59259259e-03 1.20000000e-03 8.92424242e-03\n",
      "  1.51296296e-02]\n",
      " [1.10000000e-02 5.33333333e-03 0.00000000e+00 9.78533333e-01\n",
      "  5.13333333e-03]\n",
      " [9.66666667e-03 1.00000000e-03 1.66666667e-03 1.33333333e-03\n",
      "  9.86333333e-01]\n",
      " [1.76000000e-02 0.00000000e+00 1.20000000e-03 8.00000000e-03\n",
      "  9.73200000e-01]\n",
      " [8.61580333e-01 1.78620915e-02 2.33333333e-03 4.95909091e-02\n",
      "  6.86333333e-02]\n",
      " [1.16666667e-02 0.00000000e+00 2.00000000e-03 9.61666667e-01\n",
      "  2.46666667e-02]\n",
      " [1.09090909e-03 8.61510989e-01 1.34350070e-01 2.42925942e-03\n",
      "  6.18773034e-04]\n",
      " [1.80952381e-03 8.25807213e-01 1.70239993e-01 1.52449752e-03\n",
      "  6.18773034e-04]\n",
      " [9.79402555e-01 5.04357298e-03 0.00000000e+00 2.92424242e-03\n",
      "  1.26296296e-02]\n",
      " [8.37202555e-01 6.40435730e-02 4.16666667e-03 8.04575758e-02\n",
      "  1.41296296e-02]\n",
      " [4.56709957e-03 1.69413775e-01 7.95693575e-01 2.80509804e-02\n",
      "  2.27457014e-03]\n",
      " [8.69533147e-01 6.55082194e-02 7.95238095e-03 1.21861472e-02\n",
      "  4.48201058e-02]\n",
      " [8.94578745e-01 2.02340492e-02 6.83333333e-03 1.46575758e-02\n",
      "  6.36962963e-02]\n",
      " [0.00000000e+00 1.00000000e-03 2.00000000e-03 9.96333333e-01\n",
      "  6.66666667e-04]\n",
      " [2.16666667e-02 5.16666667e-03 0.00000000e+00 9.68166667e-01\n",
      "  5.00000000e-03]\n",
      " [2.14285714e-03 8.58911633e-02 9.07876965e-01 3.81111111e-03\n",
      "  2.77903469e-04]\n",
      " [1.72000000e-02 2.84558162e-02 8.51766406e-01 2.51111111e-03\n",
      "  1.00066667e-01]\n",
      " [9.00432900e-04 2.01460623e-01 7.92703766e-01 4.36209150e-03\n",
      "  5.73086760e-04]\n",
      " [1.78333333e-02 5.03333333e-03 0.00000000e+00 2.16666667e-02\n",
      "  9.55466667e-01]\n",
      " [9.05000000e-02 1.79000000e-02 6.66666667e-04 8.67166667e-01\n",
      "  2.37666667e-02]\n",
      " [2.21666667e-02 0.00000000e+00 4.20000000e-03 4.16666667e-03\n",
      "  9.69466667e-01]\n",
      " [4.00000000e-03 3.10919273e-02 9.54257092e-01 8.66666667e-04\n",
      "  9.78431373e-03]\n",
      " [7.03299380e-01 5.33271709e-02 3.73809524e-03 2.20935354e-01\n",
      "  1.87000000e-02]\n",
      " [2.25194805e-02 8.87568440e-01 1.37783323e-02 7.08578309e-02\n",
      "  5.27591589e-03]\n",
      " [5.00857143e-02 1.83581029e-01 2.19634657e-01 4.96765266e-01\n",
      "  4.99333333e-02]\n",
      " [5.33333333e-03 1.16026302e-01 8.60444940e-01 1.70777778e-02\n",
      "  1.11764706e-03]\n",
      " [5.50000000e-03 9.10000000e-03 1.03666667e-02 1.33333333e-03\n",
      "  9.73700000e-01]\n",
      " [4.24444444e-03 5.95349631e-02 9.29419854e-01 8.66666667e-04\n",
      "  5.93407218e-03]\n",
      " [2.33938095e-01 1.10588736e-01 7.27568774e-02 5.71174603e-02\n",
      "  5.25598831e-01]\n",
      " [5.24242424e-03 2.00000000e-03 0.00000000e+00 9.85757576e-01\n",
      "  7.00000000e-03]\n",
      " [9.16666667e-03 7.60000000e-03 1.18666667e-02 1.33333333e-03\n",
      "  9.70033333e-01]\n",
      " [2.23566047e-01 1.18077585e-01 8.08168498e-03 6.37497760e-01\n",
      "  1.27769231e-02]\n",
      " [6.38666667e-02 2.78095238e-02 4.14158730e-02 2.05079365e-02\n",
      "  8.46400000e-01]\n",
      " [3.76623377e-04 7.78934903e-01 1.93957715e-01 1.31705293e-02\n",
      "  1.35602296e-02]\n",
      " [6.39682540e-03 1.83171595e-01 7.41323456e-01 1.65111111e-02\n",
      "  5.25970121e-02]\n",
      " [8.91888269e-01 3.11150016e-02 4.95238095e-03 2.82424242e-03\n",
      "  6.92201058e-02]\n",
      " [9.09090909e-05 1.79751960e-01 8.11600469e-01 2.62875817e-03\n",
      "  5.92790347e-03]\n",
      " [9.51721603e-01 2.03530968e-02 4.95238095e-03 9.41948052e-03\n",
      "  1.35534392e-02]\n",
      " [7.77575758e-03 8.75000000e-02 2.13333333e-02 8.72524242e-01\n",
      "  1.08666667e-02]\n",
      " [8.03809524e-03 1.99995132e-01 7.25747539e-01 5.13777778e-02\n",
      "  1.48414566e-02]\n",
      " [8.16937908e-01 5.48880174e-02 2.57333333e-02 9.77777778e-03\n",
      "  9.26629630e-02]\n",
      " [2.46666667e-03 3.43425108e-02 9.58262064e-01 4.81111111e-03\n",
      "  1.17647059e-04]\n",
      " [2.09523810e-03 9.06077814e-01 8.08265349e-02 9.52449752e-03\n",
      "  1.47591589e-03]\n",
      " [9.08805916e-01 1.97720798e-02 7.69230769e-05 5.76385281e-02\n",
      "  1.37065527e-02]\n",
      " [4.13766234e-02 8.70191771e-01 3.58454781e-02 3.22435451e-02\n",
      "  2.03425826e-02]\n",
      " [3.25000000e-02 1.35666667e-02 0.00000000e+00 9.30000000e-02\n",
      "  8.60933333e-01]\n",
      " [1.13704329e-01 4.04518797e-02 1.66666667e-03 8.33643791e-01\n",
      "  1.05333333e-02]\n",
      " [0.00000000e+00 1.00000000e-03 1.00000000e-03 9.98000000e-01\n",
      "  0.00000000e+00]\n",
      " [9.72820202e-01 3.25925926e-03 3.20000000e-03 3.59090909e-03\n",
      "  1.71296296e-02]\n",
      " [6.16305916e-02 7.59700351e-01 1.24889279e-01 7.88164038e-03\n",
      "  4.58981381e-02]\n",
      " [3.36507937e-03 3.11978933e-01 6.70140693e-01 1.05843137e-02\n",
      "  3.93098039e-03]\n",
      " [3.77000000e-02 1.59857614e-01 7.62475720e-01 2.13333333e-03\n",
      "  3.78333333e-02]\n",
      " [7.57575758e-04 2.91101228e-01 7.03768110e-01 3.80000000e-03\n",
      "  5.73086760e-04]\n",
      " [6.16813666e-01 1.26729080e-01 4.24369565e-02 4.84242424e-02\n",
      "  1.65596055e-01]\n",
      " [4.99090909e-02 2.00000000e-03 4.40000000e-03 1.09090909e-03\n",
      "  9.42600000e-01]\n",
      " [9.16802555e-01 1.60897268e-02 3.28717949e-03 2.13242424e-02\n",
      "  4.24962963e-02]\n",
      " [1.00000000e-02 0.00000000e+00 0.00000000e+00 2.00000000e-03\n",
      "  9.88000000e-01]\n",
      " [5.39523810e-02 3.51169591e-02 3.66666667e-03 9.03597327e-01\n",
      "  3.66666667e-03]\n",
      " [1.75000000e-02 2.61763374e-01 6.73303293e-01 2.33333333e-03\n",
      "  4.51000000e-02]\n",
      " [7.74555556e-01 9.64814815e-03 4.33333333e-03 1.54666667e-02\n",
      "  1.95996296e-01]\n",
      " [1.02857143e-02 3.15000000e-02 6.50000000e-03 9.46714286e-01\n",
      "  5.00000000e-03]\n",
      " [1.23376623e-03 3.74727544e-01 6.16436729e-01 6.48431373e-03\n",
      "  1.11764706e-03]\n",
      " [1.13442857e-01 4.76359613e-01 2.51084196e-01 1.05333333e-01\n",
      "  5.37800000e-02]]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "proba = inf_pipeline.predict_proba(X)\n",
    "L = [0,0,0,0,0]\n",
    "for x in proba : \n",
    "    a= 0\n",
    "    index = []\n",
    "    for ind,i in enumerate(x) :\n",
    "        if i > 0.38 : \n",
    "            a+=1\n",
    "            index.append(ind)\n",
    "    if a >=2 : \n",
    "        for j in index : \n",
    "            L[j] +=1  \n",
    "print(proba)  \n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved\n"
     ]
    }
   ],
   "source": [
    "submission_name = \"submission_9.csv\"\n",
    "submission_dataframe = pd.DataFrame(columns=[\"Id\",\"Category\"])\n",
    "submission_dataframe[\"Id\"] = X_test.index + 101\n",
    "y_test_pred = inf_pipeline.predict(X_test)\n",
    "submission_dataframe[\"Category\"] = y_test_pred\n",
    "submission_dataframe.to_csv(os.path.join(os.getcwd(),submission_name),index=False)\n",
    "print(\"file saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.45309234, 0.3221806 , 0.2876852 , 0.26943736, 0.34965186,\n",
      "       0.30838156, 0.3356122 , 0.47582202, 0.78869152, 0.93373723,\n",
      "       0.86713185, 1.18293099, 1.01818452, 0.78337879, 0.78320885,\n",
      "       1.08181181, 1.2430222 , 1.23056765, 1.49540677, 1.45445552,\n",
      "       1.49117894, 1.2536696 , 1.23360205, 1.57862501]), 'std_fit_time': array([0.28459516, 0.0151199 , 0.02373994, 0.00373843, 0.09258465,\n",
      "       0.05018674, 0.01295467, 0.16374434, 0.12495868, 0.1145843 ,\n",
      "       0.08713594, 0.34805971, 0.36612416, 0.03662771, 0.03888772,\n",
      "       0.39483468, 0.01911271, 0.01015333, 0.31840429, 0.24925409,\n",
      "       0.23196023, 0.01827378, 0.01057265, 0.46422141]), 'mean_score_time': array([0.02025881, 0.01911902, 0.01887088, 0.0182241 , 0.05013237,\n",
      "       0.021106  , 0.02037144, 0.04225116, 0.04688945, 0.04542885,\n",
      "       0.05473356, 0.04943643, 0.04500446, 0.0502439 , 0.04242992,\n",
      "       0.05279126, 0.07149911, 0.0725842 , 0.0720274 , 0.12420115,\n",
      "       0.12557187, 0.07177119, 0.07197719, 0.08898726]), 'std_score_time': array([0.00274371, 0.00141454, 0.00076034, 0.00080699, 0.05014803,\n",
      "       0.00285012, 0.00084739, 0.03822611, 0.00708835, 0.00456669,\n",
      "       0.03235564, 0.00575791, 0.00314647, 0.0172771 , 0.00137107,\n",
      "       0.02211964, 0.00213306, 0.00298182, 0.00442506, 0.07093443,\n",
      "       0.11441263, 0.00138577, 0.00187628, 0.03707719]), 'param_classifier__max_depth': masked_array(data=[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 5],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value=999999), 'param_classifier__max_features': masked_array(data=['sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value=np.str_('?'),\n",
      "            dtype=object), 'param_classifier__min_samples_leaf': masked_array(data=[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "                   2, 2, 2, 2, 2, 2],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value=999999), 'param_classifier__min_samples_split': masked_array(data=[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "                   2, 2, 2, 2, 2, 2],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value=999999), 'param_classifier__n_estimators': masked_array(data=[100, 100, 100, 100, 100, 100, 100, 100, 300, 300, 300,\n",
      "                   300, 300, 300, 300, 300, 500, 500, 500, 500, 500, 500,\n",
      "                   500, 500],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value=999999), 'param_dataAugment__noise_factor': masked_array(data=[0.0, 0.01, 0.03, 0.05, 0.08, 0.1, 0.15, 0.2, 0.0, 0.01,\n",
      "                   0.03, 0.05, 0.08, 0.1, 0.15, 0.2, 0.0, 0.01, 0.03,\n",
      "                   0.05, 0.08, 0.1, 0.15, 0.2],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value=1e+20), 'params': [{'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100, 'dataAugment__noise_factor': 0.0}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100, 'dataAugment__noise_factor': 0.01}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100, 'dataAugment__noise_factor': 0.03}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100, 'dataAugment__noise_factor': 0.05}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100, 'dataAugment__noise_factor': 0.08}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100, 'dataAugment__noise_factor': 0.1}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100, 'dataAugment__noise_factor': 0.15}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100, 'dataAugment__noise_factor': 0.2}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 300, 'dataAugment__noise_factor': 0.0}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 300, 'dataAugment__noise_factor': 0.01}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 300, 'dataAugment__noise_factor': 0.03}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 300, 'dataAugment__noise_factor': 0.05}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 300, 'dataAugment__noise_factor': 0.08}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 300, 'dataAugment__noise_factor': 0.1}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 300, 'dataAugment__noise_factor': 0.15}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 300, 'dataAugment__noise_factor': 0.2}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 500, 'dataAugment__noise_factor': 0.0}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 500, 'dataAugment__noise_factor': 0.01}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 500, 'dataAugment__noise_factor': 0.03}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 500, 'dataAugment__noise_factor': 0.05}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 500, 'dataAugment__noise_factor': 0.08}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 500, 'dataAugment__noise_factor': 0.1}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 500, 'dataAugment__noise_factor': 0.15}, {'classifier__max_depth': 5, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 500, 'dataAugment__noise_factor': 0.2}], 'split0_test_score': array([0.9 , 0.9 , 0.9 , 0.95, 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 ,\n",
      "       0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 ,\n",
      "       0.9 , 0.95]), 'split1_test_score': array([0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.9 , 0.9 , 0.95, 0.95, 0.95,\n",
      "       0.95, 0.9 , 0.9 , 0.95, 0.9 , 0.9 , 0.95, 1.  , 0.95, 0.95, 0.95,\n",
      "       0.9 , 0.9 ]), 'split2_test_score': array([1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.95, 1.  , 1.  , 1.  , 1.  ,\n",
      "       1.  , 1.  , 1.  , 0.9 , 0.95, 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
      "       0.95, 1.  ]), 'split3_test_score': array([0.9 , 0.9 , 0.85, 0.9 , 0.9 , 0.95, 0.9 , 0.85, 0.9 , 0.9 , 0.9 ,\n",
      "       0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 , 0.9 ,\n",
      "       0.9 , 0.9 ]), 'split4_test_score': array([0.95, 0.95, 0.95, 0.95, 0.9 , 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,\n",
      "       0.95, 0.9 , 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,\n",
      "       0.95, 0.95]), 'mean_test_score': array([0.94, 0.94, 0.93, 0.95, 0.93, 0.95, 0.92, 0.92, 0.94, 0.94, 0.94,\n",
      "       0.94, 0.92, 0.93, 0.92, 0.92, 0.93, 0.94, 0.95, 0.94, 0.94, 0.94,\n",
      "       0.92, 0.94]), 'std_test_score': array([0.03741657, 0.03741657, 0.0509902 , 0.03162278, 0.04      ,\n",
      "       0.03162278, 0.0244949 , 0.0509902 , 0.03741657, 0.03741657,\n",
      "       0.03741657, 0.03741657, 0.04      , 0.04      , 0.0244949 ,\n",
      "       0.0244949 , 0.04      , 0.03741657, 0.04472136, 0.03741657,\n",
      "       0.03741657, 0.03741657, 0.0244949 , 0.03741657]), 'rank_test_score': array([ 4,  4, 15,  1, 15,  1, 19, 19,  4,  4,  4,  4, 19, 17, 19, 19, 17,\n",
      "        4,  1,  4,  4,  4, 19,  4], dtype=int32), 'split0_train_score': array([0.9875, 0.9875, 0.9875, 0.9875, 0.9875, 0.9875, 0.975 , 0.9625,\n",
      "       0.9875, 0.9875, 0.9875, 0.9875, 0.9875, 0.9875, 0.9875, 0.975 ,\n",
      "       0.9875, 0.9875, 0.9875, 0.9875, 0.9875, 0.9875, 0.9875, 0.975 ]), 'split1_train_score': array([1.    , 0.9875, 0.9875, 0.9875, 0.9625, 0.9625, 0.9875, 0.9625,\n",
      "       0.9875, 0.9875, 0.9875, 0.975 , 0.975 , 0.975 , 0.975 , 0.975 ,\n",
      "       0.9875, 0.9875, 0.9875, 0.9875, 0.9875, 0.975 , 0.975 , 0.9625]), 'split2_train_score': array([0.9875, 0.9875, 0.9875, 0.9875, 0.975 , 0.975 , 0.9625, 0.9625,\n",
      "       0.9875, 0.9875, 0.9875, 0.975 , 0.975 , 0.9875, 0.9875, 0.9875,\n",
      "       0.9875, 0.9875, 0.9875, 0.9875, 0.975 , 0.975 , 0.975 , 0.975 ]), 'split3_train_score': array([0.9875, 0.9875, 0.9875, 0.975 , 0.975 , 0.9875, 0.975 , 0.975 ,\n",
      "       0.9875, 0.9875, 0.9875, 0.9875, 0.9875, 0.975 , 0.9875, 0.9875,\n",
      "       0.9875, 0.9875, 0.9875, 0.975 , 0.975 , 0.975 , 0.9875, 0.9625]), 'split4_train_score': array([1.    , 1.    , 1.    , 1.    , 0.9875, 0.975 , 1.    , 1.    ,\n",
      "       1.    , 1.    , 1.    , 0.9875, 0.9875, 1.    , 0.975 , 0.9875,\n",
      "       1.    , 1.    , 1.    , 1.    , 1.    , 1.    , 0.975 , 0.975 ]), 'mean_train_score': array([0.9925, 0.99  , 0.99  , 0.9875, 0.9775, 0.9775, 0.98  , 0.9725,\n",
      "       0.99  , 0.99  , 0.99  , 0.9825, 0.9825, 0.985 , 0.9825, 0.9825,\n",
      "       0.99  , 0.99  , 0.99  , 0.9875, 0.985 , 0.9825, 0.98  , 0.97  ]), 'std_train_score': array([0.00612372, 0.005     , 0.005     , 0.00790569, 0.00935414,\n",
      "       0.00935414, 0.01274755, 0.01457738, 0.005     , 0.005     ,\n",
      "       0.005     , 0.00612372, 0.00612372, 0.00935414, 0.00612372,\n",
      "       0.00612372, 0.005     , 0.005     , 0.005     , 0.00790569,\n",
      "       0.00935414, 0.01      , 0.00612372, 0.00612372])}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           feature  importance\n",
      "20         ED_RV_vol_div_ES_LV_vol    0.028274\n",
      "94         ES_RV_vol_div_ES_MY_vol    0.028065\n",
      "109        ES_LV_vol_div_ED_RV_vol    0.027842\n",
      "44         ED_LV_vol_div_ES_LV_vol    0.027359\n",
      "118        ES_LV_vol_div_ES_MY_vol    0.027125\n",
      "..                             ...         ...\n",
      "27   ED_RV_border_div_ED_LV_border    0.000270\n",
      "97      ES_RV_border_div_ED_RV_vol    0.000202\n",
      "36   ED_RV_border_div_body_surface    0.000165\n",
      "77      ED_MY_border_div_ED_MY_vol    0.000130\n",
      "51      ED_LV_border_div_ED_LV_vol    0.000119\n",
      "\n",
      "[169 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "f_importance = grid_search.best_estimator_.named_steps[\"classifier\"].feature_importances_\n",
    "f_name = grid_search.best_estimator_.named_steps[\"normaliser\"].get_feature_names_out()\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": f_name,\n",
    "    \"importance\": f_importance\n",
    "})\n",
    "feature_importance.sort_values(\"importance\", ascending=False, inplace=True)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below two cells to save the results : the model and the description of the method used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"Data augmentation cleaned + MinmaxScaler + Randomforest with Gridsearch.\" \n",
    "other_params = \"The features are just the volume of each segmentation + body surface + all the possible ratios.\"\n",
    "name_folder = \"RF_data_aug_noise_pipeline\"\n",
    "feature_used = f_name\n",
    "informationDict = {\n",
    "    \"description\": description,\n",
    "    \"model parameters\" : best_params,\n",
    "    \"features used\" : feature_used,\n",
    "    \"mean test accuracy with best params\" : mean_valid_score ,\n",
    "    \"std  test with best params\" : std_valid_score,\n",
    "    \"mean train accuracy with best params\" : mean_train_score,\n",
    "    \"std train best params\" : std_train_score,\n",
    "    \"other parms\" : other_params,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: /Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/output/RF_data_aug_noise_pipeline/pipeline_01-08-15.pkl\n",
      "Information about the model saved to: /Users/rplanchon/Documents/telecom/IMA/S2/IMA205/Challenge/CardiacPathoPrediction/output/RF_data_aug_noise_pipeline/params.txt\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create a timestamp\n",
    "currentDateTime = datetime.now()\n",
    "\n",
    "# Get the base directory (current directory)\n",
    "BASE_DIR = os.getcwd()\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "RESULT_DIR = os.path.join(BASE_DIR,\"output\")\n",
    "if not os.path.exists(RESULT_DIR):\n",
    "    os.mkdir(RESULT_DIR)\n",
    "    print(f\"Directory created: {RESULT_DIR}\")\n",
    "\n",
    "# Create a folder named 'pipeline_<timestamp>' in the current directory\n",
    "dir_name = name_folder\n",
    "dir_path = os.path.join(RESULT_DIR, dir_name)\n",
    "\n",
    "# If the directory doesn't exist, create it\n",
    "if not os.path.exists(dir_path):\n",
    "    os.mkdir(dir_path)\n",
    "    print(f\"Directory created: {dir_path}\")\n",
    "\n",
    "# Save the model inside this new folder\n",
    "model_filename = 'pipeline_' + currentDateTime.strftime(\"%H-%M-%S\") + '.pkl'\n",
    "model_path = os.path.join(dir_path, model_filename)\n",
    "\n",
    "# This is where you'd have your model defined\n",
    "joblib.dump(grid_search, model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "\n",
    "# Saving feature importance : \n",
    "feature_importance_filename ='pipeline_' + currentDateTime.strftime(\"%H-%M-%S\") + '.csv' \n",
    "feature_importance_dir = os.path.join(dir_path,feature_importance_filename)\n",
    "feature_importance.to_csv(feature_importance_dir)\n",
    "\n",
    "\n",
    "# SAVING Description \n",
    "dict_filename = 'params.txt'\n",
    "dict_path = os.path.join(dir_path, dict_filename)\n",
    "\n",
    "with open(dict_path, 'w') as f:\n",
    "    for key, val in informationDict.items():\n",
    "        f.write(f\"{key} : {val}\\n\")\n",
    "print(f\"Information about the model saved to: {dict_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
